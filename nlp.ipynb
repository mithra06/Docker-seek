{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b82eaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3482f272",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"data/dataset-seek.csv\")\n",
    "df1=df.iloc[:520,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f3d177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5bec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1602ab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.company.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f28f7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b68d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de3c4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.salary.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8b8511",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "title_counts=df1.title.value_counts()\n",
    "title_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b6505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_counts.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c71847",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45460422",
   "metadata": {},
   "source": [
    "##salary cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc988b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['salary']=df1['salary'].fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271d446b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['salary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a06ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['sal']=df1.astype({'salary': 'str'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e59a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['sal1']=df1['salary'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a65a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['sal1']=df1['sal1'].apply(lambda x: x.replace('k','').replace('$',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d148cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['sal1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28578458",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Salary Estimate']=(df1['sal1'].apply(lambda x:x.split('+')))  \n",
    "df1['Salary Estimate']=(df1['sal1'].apply(lambda x:x.strip('$')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d59e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Salary Estimate']=(df1['sal1'].apply(lambda x:x.strip('$')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56698a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5e3ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Salary Estimate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f36ff0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbfa8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['sal1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844474bf",
   "metadata": {},
   "outputs": [],
   "source": [
    " df1['Salary-Estimate1']=df1['sal1'].apply(lambda x:x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea40199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba31714",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Salary-Estimate1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670c5ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['salary_estimate']=df1['sal1'].apply(lambda x:x.split('+')[0])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d853f528",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['salary_estimate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85143f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df1['Salary-Estimate1'][:][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3407ca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['salary_estimate'][:][0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fe8d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['salary_min']=(df1['Salary-Estimate1'].apply(lambda x:x[:][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015fa9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['salary_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f7bfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['min_salary'] =  df1['salary_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de94316",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Salary-max']=(df1['Salary-Estimate1'].apply(lambda x:x[:][2:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1557b2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Salary-max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c94e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['value1'] =  df1['Salary-max'].str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3062ad1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e9687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df1.drop(['Salary Estimate', 'Salary-Estimate1','sal1','sal','salary_estimate','salary_min','Salary-max'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5aac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dd5dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df1['min_salary']=df1['min_salary'].apply(lambda x:x.split(',')[0])\n",
    "df1['value2']=df1['value1'].astype(str).apply(lambda x:x.split(',')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8d38ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['value2']=df1['value1'].astype(str).apply(lambda x:x.split(',')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec0ea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60bfd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df1.drop(['value1'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63052c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['min_salary1']=df1['min_salary'].apply(lambda x:x.split('-')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d6c8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_salary1=df1['min_salary1']\n",
    "df1['min_salary1'].replace(to_replace = 'Base',value='103',inplace=True)\n",
    "df1['min_salary1'].replace(to_replace = 'Attractive',value='190',inplace=True)\n",
    "df1['min_salary1'].replace(to_replace = 'Up',value='100',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac67c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['min_salary1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d38801",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['value2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a78838",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1['value2']=='Super']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a44ebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb505f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df1.drop(['salary','min_salary'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f181c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.rename(columns = {'value2':'max_salary','min_salary1':'min_salary'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f91c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['max_salary']=pd.to_numeric(df1['max_salary'], errors='coerce')\n",
    "df1['min_salary']=pd.to_numeric(df1['min_salary'], errors='coerce')\n",
    "df1['max_salary']=df1['max_salary'].fillna(method='bfill')\n",
    "df1['min_salary1']=df1['min_salary'].fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c417c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['min_salary']=pd.to_numeric(df1['min_salary'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0697e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['max_salary']=df1['max_salary'].fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d83024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['min_salary1']=df1['min_salary'].fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6008bab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['max_salary'].value_counts()\n",
    "#((df1['value2']=='Super')/df1['min_salary1']).mul(100).round(2).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243a1efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['min_salary']=df1['min_salary'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb119734",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['avg_salary'] = (df1.min_salary+df1.max_salary)//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d63acbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de002616",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52900114",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['min_salary'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234e6bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['max_salary'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940df49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df1.drop(['min_salary1'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f61054",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20649866",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.title.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b12257",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873a2aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_by_state = df1.groupby(['location'])[['min_salary']].count()\n",
    "job_by_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d181c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "category_col =['class', 'location','title','company']  \n",
    "#df1=df1[category_col]\n",
    "labelEncoder = preprocessing.LabelEncoder() \n",
    "  \n",
    "mapping_dict ={} \n",
    "for col in category_col: \n",
    "    df1[col] = labelEncoder.fit_transform(df1[col]) \n",
    "  \n",
    "    le_name_mapping = dict(zip(labelEncoder.classes_, \n",
    "                        labelEncoder.transform(labelEncoder.classes_))) \n",
    "mapping_dict[col]= le_name_mapping \n",
    "print(mapping_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5b8252",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a145c67c",
   "metadata": {},
   "source": [
    "company_mapping = {'Stockland':0, 'University of Technology Sydney':1,'Susquehanna Pacific Pty Ltd':2,\n",
    "                  'CPB Contractors Pty Limited':3,'AC3 Pty Limited':4,'TheDriveGroup':5,\n",
    "                   'The Onset': 6, 'SEEK Limited': 7, 'PRA': 8, 'Talent Insights Group Pty Ltd': 9,\n",
    "                   'Pearson Australia': 10, 'Westpac Group': 11, 'ASIC': 12,'The Argyle Network':13,\n",
    "                  'Correlate Resources':14,'Bluefin Resources Pty Limited':15}\n",
    "dummy.company = df1.company.map(company_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc16610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df1.drop(['days_before','max_salary','min_salary'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637f6970",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15cea77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714f5cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c721a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_sentences(sentence):\n",
    "    \n",
    "    #sentence = sentence.replace(\"[^a-zA-Z\\d\\s.]\", \" \")    \n",
    "    line = re.sub(r\"[^a-zA-Z\\d\\s.]+\", \" \", str(sentence))\n",
    "    tokens = line.replace(\".\", \" \").split()\n",
    "    stemmed_tokens = [stemmer.stem(token.strip()) for token in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f9c871",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['summary_stem'] = df1['description'].apply(stem_sentences)\n",
    "df1['summary_stem'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac350467",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1=df1.drop(['description','max_salary','min_salary'],axis=1)\n",
    "y = df1['avg_salary']\n",
    "X = df1.drop(columns=['avg_salary'])\n",
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2df2487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbfb573",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = list(ENGLISH_STOP_WORDS)\n",
    "# add own custom stopwords\n",
    "word_list = ['thi', 'skill','click', 'job', 'role', 'cover', 'letter','resume', 'apply', 'button', 'abil'\n",
    "             'quot', 'pleas', 'requir', 'phone', 'call', 'provid', 'includ', 'success', 'com', 'au', 'strong']\n",
    "for w in word_list:\n",
    "    custom_stop_words.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f62c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "def generate_word_cloud(words, freq=False):\n",
    "    \n",
    "    # if not frequency, the data is given as panda series\n",
    "    # then need to do some data processing first\n",
    "    if freq==False:\n",
    "\n",
    "        stem_words = ' '\n",
    "\n",
    "        for val in words:\n",
    "            #val = str(val).lower()\n",
    "            #line = re.sub(r\"[^a-zA-Z\\d\\s.]+\", \" \", str(val))\n",
    "            tokens = str(val).replace(\".\", \" \").split()\n",
    "            stemmed_tokens = [token.strip() for token in tokens] \n",
    "            stem_words = ' '.join(stemmed_tokens)\n",
    "\n",
    "    #'--------------------------------------------------'\n",
    "    # generate word could with some defined parameters\n",
    "    wcloud = WordCloud(width = 480, height = 480,\n",
    "                    background_color ='white', \n",
    "                    stopwords = custom_stop_words, \n",
    "                    min_font_size = 8)\n",
    "    \n",
    "    #'--------------------------------------------------'\n",
    "    # now generate word clound based\n",
    "    if freq:\n",
    "        wcloud.generate_from_frequencies(words)\n",
    "    else:\n",
    "        wcloud.generate(stem_words) \n",
    "        \n",
    "    #'--------------------------------------------------'\n",
    "    # plot the WordCloud image                        \n",
    "    plt.figure(figsize = (8, 8), facecolor = None) \n",
    "    plt.imshow(wcloud) \n",
    "    plt.axis(\"off\") \n",
    "    plt.tight_layout(pad = 0) \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee1a1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.summary_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78647f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_word_cloud(X.summary_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0205f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_test(vec):\n",
    "    \n",
    "    # Create document-term matrices - NLP\n",
    "    #------------------------------------------------------------\n",
    "    # Fitting and Transfrom the vectorizer on training data\n",
    "    X_train_vec = vec.fit_transform(X_train['summary_stem'])\n",
    "    print('Features: ', X_train_vec.shape[1])\n",
    "\n",
    "    # Transform on test data\n",
    "    X_test_vec = vec.transform(X_test['summary_stem'])\n",
    "    #------------------------------------------------------------\n",
    "    \n",
    "    # Create dataframe after document-term matrices has been created\n",
    "    #------------------------------------------------------------\n",
    "    X_train_vec_df = pd.DataFrame(X_train_vec.todense(), columns=vec.get_feature_names())\n",
    "    X_test_vec_df = pd.DataFrame(X_test_vec.todense(), columns=vec.get_feature_names())\n",
    "    #------------------------------------------------------------\n",
    "    \n",
    "    # Extract top 10 and EDA\n",
    "    #------------------------------------------------------------\n",
    "    # top 10 words with highest frequency - work experience, management, data, team\n",
    "    words = X_train_vec_df.sum().sort_values(ascending=False) #.head(20)\n",
    "    #print(words)\n",
    "    \n",
    "    # plot top 10 words\n",
    "    #words.plot(kind='bar', figsize=(15, 5))\n",
    "    generate_word_cloud(words.to_dict(), True)\n",
    "    #------------------------------------------------------------\n",
    "    \n",
    "    # Combine to create the full set of X\n",
    "    #------------------------------------------------------------\n",
    "    # remove stem columns as it now has been processed\n",
    "    mask_train_df = X_train.drop('summary_stem',axis=1).reset_index(drop=True)\n",
    "    mask_test_df = X_test.drop('summary_stem',axis=1).reset_index(drop=True)\n",
    "\n",
    "    # time to put the data back to the main X_train and X_test\n",
    "    X_train_dtm = pd.concat([mask_train_df, X_train_vec_df], axis=1)\n",
    "    X_test_dtm = pd.concat([mask_test_df, X_test_vec_df], axis=1)\n",
    "    #------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    # Kbest Feature Selection after NLP\n",
    "    #----------------------------------------------------------\n",
    "    # build the selector (build one with each score type)\n",
    "    skb_f = SelectKBest(f_classif) # default is k=10)\n",
    "\n",
    "    # train the selector on data\n",
    "    skb_f.fit(X_train_dtm, y_train)\n",
    "\n",
    "    # examine results\n",
    "    kbest = pd.DataFrame({'variable': X_train_dtm.columns.values.tolist(),\n",
    "                         'score': list(skb_f.scores_)}).sort_values('score', ascending = False)\n",
    "    \n",
    "    kbest.columns = ['kBest_features', 'kBest_score']    \n",
    "    kbest.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # plot top 10 features\n",
    "    kbest.head(20).plot(kind='bar', x='kBest_features', y='kBest_score', figsize=(15,5), grid=True)\n",
    "    \n",
    "    X_Ktrain = X_train_dtm[kbest.kBest_features]\n",
    "    X_Ktest = X_test_dtm[kbest.kBest_features]\n",
    "\n",
    "    return (X_Ktrain, X_Ktest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be4c415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "tvec = TfidfVectorizer(ngram_range=(2,3), stop_words=custom_stop_words, max_features=1000)\n",
    "\n",
    "\n",
    "#X = TfidV.fit_transform(X)\n",
    "#X=tvec.fit_transform(X)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n",
    "\n",
    "X_train, X_test = tokenize_test(tvec)\n",
    "\n",
    "model=RandomForestRegressor(max_depth=50,n_estimators=100, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883ac84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "errors = abs(y_pred - y_test)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c901ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "mape = 100 * (errors / y_test)\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97f9622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('seek_predict11.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
